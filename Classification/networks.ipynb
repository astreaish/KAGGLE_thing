{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Classifier, Layer, Convolution\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from sklearn import preprocessing as prep\n",
    "import sklearn.cross_validation as crval\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GENERAL\n",
    "def test_and_answers(solver, test):\n",
    "    print solver\n",
    "    print shape(test)\n",
    "    accuracy = solver.predict(test)\n",
    "    print 'Saving answers...'\n",
    "    fp=open('answers.txt','w')\n",
    "    fp.write('Point_ID,Output\\n')\n",
    "    i=1\n",
    "    for a in accuracy:\n",
    "        #fp.write(str(i)+','+a+'\\n')\n",
    "        fp.write(str(i)+','+a)\n",
    "        i+=1\n",
    "    fp.close()\n",
    "    print 'Done.'\n",
    "    \n",
    "    \n",
    "\n",
    "def compute_accuracy_val(target, y):\n",
    "    cont=0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] is (target[i]):\n",
    "            cont+=1\n",
    "    return round(float(cont*(pow(len(target),-1))),4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TENSORFLOW\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    #layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    #layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    #out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1962, 265)\n",
      "(1962, 1)\n",
      "(1963, 265)\n"
     ]
    }
   ],
   "source": [
    "#LOADING DATA ..... \n",
    "    \n",
    "X=[]\n",
    "fp=open('class_train_in.csv','r')\n",
    "trash=fp.readline()\n",
    "#print trash\n",
    "lines=fp.readlines()\n",
    "# print lines[0]\n",
    "for line in lines:\n",
    "    data=line.split(',')\n",
    "    cleaned_data=data[1:len(data)]\n",
    "    X.append(cleaned_data)\n",
    "print shape(X)\n",
    "\n",
    "      \n",
    "y=[]\n",
    "fp=open('class_train_out.csv','r')\n",
    "trash=fp.readline()\n",
    "#print trash\n",
    "lines=fp.readlines()\n",
    "# print lines[0]\n",
    "for line in lines:\n",
    "    data=line.split(',')\n",
    "    cleaned_data=data[1:len(data)]\n",
    "    y.append(cleaned_data)\n",
    "print shape(y)\n",
    "    \n",
    "    \n",
    "X_test=[]\n",
    "fp=open('class_test_in.csv','r')\n",
    "trash=fp.readline()\n",
    "lines=fp.readlines()\n",
    "for line in lines:\n",
    "    data=line.split(',')\n",
    "    cleaned_data=data[1:len(data)]\n",
    "    X_test.append(cleaned_data)\n",
    "print shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1962, 265)\n",
      "(1962, 1)\n",
      "(1963, 265)\n"
     ]
    }
   ],
   "source": [
    "# SET UP DATA ARRAYS\n",
    "X=np.asarray(X)\n",
    "X=X.astype(np.float)\n",
    "X=np.ravel(X)\n",
    "X= X.reshape(1962, 265)\n",
    "print shape(X)\n",
    "\n",
    "y=np.asarray(y)\n",
    "y=np.ravel(y).reshape(1962, 1)\n",
    "print shape(y)\n",
    "\n",
    "X_test=np.asarray(X_test)\n",
    "X_test=X_test.astype(np.float)\n",
    "X_test=np.ravel(X_test)\n",
    "X_test= X_test.reshape(1963, 265)\n",
    "print shape(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train:\n",
      "it had mean: 93.6561498663 and std:24634.8063775\n",
      "NOW it has mean: -7.3728733857e-18 and std: 1.0\n",
      "X test:\n",
      "it had mean: 93.7580887936 and std:24628.007464\n",
      "NOW it has mean: -1.45060291886e-17 and std: 1.0\n"
     ]
    }
   ],
   "source": [
    "#NORMALIZING DATA\n",
    "print 'X train:'\n",
    "std_scal=prep.StandardScaler()\n",
    "X_scaled=std_scal.fit_transform(X)\n",
    "print('it had mean: ' +str(mean(X))+' and std:'+ str(std(X)))\n",
    "print('NOW it has mean: '+str(mean(X_scaled))+' and std: '+str(std(X_scaled)))\n",
    "\n",
    "X_tr = X_scaled #normalized training data\n",
    "y_tr = y\n",
    "\n",
    "\n",
    "print 'X test:'\n",
    "selX=X_test\n",
    "std_scal=prep.StandardScaler()\n",
    "selX_scaled=std_scal.fit_transform(selX)\n",
    "print('it had mean: ' +str(mean(selX))+' and std:'+ str(std(selX)))\n",
    "print('NOW it has mean: '+str(mean(selX_scaled))+' and std: '+str(std(selX_scaled)))\n",
    "\n",
    "X_test = selX_scaled #normalized testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUFFLE: Lets mess up things a little bit\n",
      "total matrix of data:(1962, 265)\n",
      "SPLIT: create of train/test/val sets\n",
      "Done. Here the sizes: \n",
      "Train: (1569, 265);\tVal: (393, 265)\n"
     ]
    }
   ],
   "source": [
    "# SHUFFLE AND SPLIT\n",
    "print('SHUFFLE: Lets mess up things a little bit')\n",
    "selX_scaled=X\n",
    "sely=y\n",
    "indexes=[]\n",
    "shuffly=y\n",
    "shufflX=X\n",
    "# assign an id to each sample\n",
    "for i in range(len(sely)):\n",
    "    indexes.append(i)\n",
    "# shuffle idxs\n",
    "np.random.shuffle(indexes)\n",
    "# build X and y shuffled\n",
    "for i in range(len(indexes)):\n",
    "    shuffly[i]=sely[i]\n",
    "    shufflX[i, :]= selX_scaled[i,:]\n",
    "print('total matrix of data:'+str(shape(shufflX)))\n",
    "\n",
    "\n",
    "print('SPLIT: create of train/test/val sets')\n",
    "trainData, valData, trainLab, valLab = crval.train_test_split(shufflX, shuffly, test_size=0.2)\n",
    "\n",
    "#trainData, valData, trainLab, valLab = crval.train_test_split(shufflX, shuffly, test_size=0.2)\n",
    "#print(shape(otherData))\n",
    "print('Done. Here the sizes: ')\n",
    "print('Train: '+str(shape(trainData))+';\\tVal: '+str(shape(valData)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEURAL NET, TRY\n",
    "#from scikit.neuralnetwork import MLPClassifier\n",
    "from sknn.mlp import Classifier, Layer, Convolution\n",
    "\n",
    "#class sknn.mlp.Classifier(layers, warning=None, parameters=None, random_state=None, learning_rule=u'sgd', learning_rate=0.01, learning_momentum=0.9, normalize=None, regularize=None, weight_decay=None, dropout_rate=None, batch_size=1, n_iter=None, n_stable=10, f_stable=0.001, valid_set=None, valid_size=0.0, loss_type=None, callback=None, debug=False, verbose=None, **params)\n",
    "\n",
    "##mlp = Classifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, learning_rate='constant', learning_rate_init=0.001)\n",
    "#mlp = Classifier(solver='lbfgs', alpha=1e-5, random_state=1, learning_rule='constant', learning_rate=0.001)\n",
    "#mlp.fit(trainData, trainLab)\n",
    "\n",
    "#MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "    #epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate='constant', learning_rate_init=0.001, max_iter=200,\n",
    "    #momentum=0.9, nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
    "    #validation_fraction=0.1, verbose=False, warm_start=False)\n",
    "\n",
    "typeslin = ['Rectifier', 'Sigmoid', 'Tanh']\n",
    "typesnonlin = ['ExpLin']\n",
    "typesclass = ['Linear', 'Softmax']\n",
    "    \n",
    "layers=[\n",
    "        Layer(\"Sigmoid\", units=100),\n",
    "        #Layer(\"Sigmoid\", units=50),\n",
    "        Layer(\"Rectifier\", units=100),\n",
    "        #Layer('ExpLin', units=100),\n",
    "        Layer(\"Softmax\")]\n",
    "\n",
    "#layers=[Layer('Sigmoid', units=100) for i in range(5)]\n",
    "print layers\n",
    "    \n",
    "#mlp = Classifier(layers, learning_rate=0.1, n_iter=50, regularize='dropout', random_state=1)\n",
    "mlp = Classifier(layers, learning_rate=0.1, n_iter=50, regularize='dropout', random_state=1,learning_rule='adadelta')\n",
    "mlp.fit(trainData, trainLab)\n",
    "    \n",
    "print 'Trained'\n",
    "# to be continued..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ...continuing (NN)\n",
    "# on validation\n",
    "y = mlp.predict(valData)\n",
    "cont=0\n",
    "for i in range(len(y)):\n",
    "    if y[i]==valLab[i]:\n",
    "        cont+=1\n",
    "accuracy = round(float(cont*(pow(len(valLab),-1))),4)\n",
    "print accuracy\n",
    "\n",
    "# [sig(100), rect(100)], lr=0.1, niter=50, regul=dropout, rndstate=1 -> acc=0.7968 = 4answers.csv\n",
    "# [sig(100), rect(100)], lr=0.1, niter=50, regul=dropout, rndstate=1 -> acc=\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ...continuing (NN)\n",
    "# on test\n",
    "y = mlp.predict(X_test)\n",
    "accuracy = y\n",
    "\n",
    "fp=open('answers.txt','w')\n",
    "fp.write('Point_ID,Output\\n')\n",
    "i=1\n",
    "for a in accuracy:\n",
    "    fp.write(str(i)+','+str(a[0])+'\\n')\n",
    "    i+=1\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DEFINE VARIABLES AND PARAM\n",
    "\n",
    "n_feat = len(trainData[1,:])\n",
    "n_classes = 1\n",
    "n_samples = len(trainLab)\n",
    "\n",
    "# Y = W*X+b\n",
    "x = tf.placeholder(tf.float32, [None, n_feat])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "# initialize weights and bias\n",
    "#W = tf.Variable(tf.zeros([n_feat, 2]))\n",
    "#b = tf.Variable(tf.zeros([2]))\n",
    "W = tf.Variable(tf.random_normal([n_feat, 2]))\n",
    "b = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "#trainD = tf.constant(trainData)\n",
    "#trainD = trainData.as_matrix()\n",
    "#trainL = tf.constant(trainLab)\n",
    "#print trainD.get_shape()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 10\n",
    "\n",
    "\n",
    "output = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "\n",
    "print trainData\n",
    "#with tf.Session().as_default():\n",
    "#    print x.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# METHOD 1\n",
    "\n",
    "# define loss function to minimize:\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(output), reduction_indices=[1])) #cross entropy\n",
    "loss = tf.reduce_sum(tf.pow(y - output, 2))/(2*n_samples) # mean squared error\n",
    "\n",
    "# define training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# initialize net:\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# initialize training session:\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # training\n",
    "    print 'Training...'\n",
    "    for i in range(training_epochs):\n",
    "        #batch_xs, batch_ys = trainD.next_batch(batch_size), trainL.next_batch(batch_size)\n",
    "        sess.run(train_step, feed_dict={x: trainData, y: trainLab})\n",
    "        output = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "        print output.eval(session=sess)\n",
    "        loss = tf.reduce_sum(tf.pow(y - output, 2))/(2*n_samples) # mean squared error\n",
    "        #print x\n",
    "        if not (i) % display_step:\n",
    "            c = sess.run(loss, feed_dict={x:trainData, y: trainLab})\n",
    "            print \"i = {}; \\tloss = {}\".format(i,c)\n",
    "\n",
    "\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# METHOD 2\n",
    "\n",
    "# Store layers weight & bias\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_feat, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "loss = tf.reduce_mean()\n",
    "#loss = tf.reduce_mean(tf.nn.softmax(pred))\n",
    "#train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_loss = 0.\n",
    "        total_batch = int(len(trainLab)/batch_size)\n",
    "        s = 0\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            #batch_x, batch_y = trainData[s:s+batch_size], [int(trainLab[j][0].strip()) for j in range(s,batch_size)]\n",
    "            batch_x = trainData[s:s+batch_size]\n",
    "            \n",
    "            batch_y = np.array([int(trainLab[j][0]) for j in range(s,batch_size)]).ravel().reshape(batch_size,1)\n",
    "            #batch_x, batch_y = trainData[s:s+batch_size], trainLab[s:s+batch_size]\n",
    "            print shape(batch_y)\n",
    "            #batch_x, batch_y = trainData, trainLab\n",
    "            # Run optimization op (backprop) and loss op (to get loss value)\n",
    "            _, c = sess.run([train_step, loss], feed_dict={x: batch_x, y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_loss += float(c / total_batch)\n",
    "            print c\n",
    "            s+=batch_size\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"loss =\", \"{:.9f}\".format(c)\n",
    "            #print \"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_loss)\n",
    "            #print \"epoch = {}; \\tloss = {.6f}\".format(epoch,c)\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    #a = sess.run([accuracy, correct_prediction], feed_dict={x: trainData, y: trainLab})\n",
    "    print \"Accuracy:\", accuracy.eval({x: valData, y: valLab})\n",
    "    #print \"Accuracy:\", a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "# performance evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# accuracy on validation\n",
    "print(sess.run(accuracy, feed_dict={x: valData, y: valLab}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 33884.  30110.  -2466. ..., -25072. -13751. -16103.]\n",
      " [ 34867.  28911.   -377. ..., -27951. -13897. -11782.]\n",
      " [ 35224.  30093.  -3251. ..., -25687. -15908. -12349.]\n",
      " ..., \n",
      " [ 33560.  33436.  -3373. ..., -23780. -11747. -16528.]\n",
      " [ 35432.  29254.   -749. ..., -25775. -13450. -13224.]\n",
      " [ 33047.  30679.  -1288. ..., -26765. -11867. -13894.]]\n"
     ]
    }
   ],
   "source": [
    "# Trick: Turn inputs and outputs into shared variables. \n",
    "# It's still the same thing, but we can later change the values of the shared variable \n",
    "# (to switch in the test-data later) and pymc3 will just use the new data. \n",
    "# Kind-of like a pointer we can redirect.\n",
    "# For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "\n",
    "n_feat = len(trainData[1,:])\n",
    "n_classes = 1\n",
    "n_samples = len(trainLab)\n",
    "\n",
    "\n",
    "ann_input = theano.shared(trainData)\n",
    "ann_output = theano.shared(trainLab)\n",
    "\n",
    "n_hidden = 5\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "init_1 = np.random.randn(n_feat, n_hidden)\n",
    "init_2 = np.random.randn(n_hidden, n_hidden)\n",
    "init_out = np.random.randn(n_hidden)\n",
    "#init_1 = np.random.randn(X.shape[1], n_hidden)\n",
    "#init_2 = np.random.randn(n_hidden, n_hidden)\n",
    "#init_out = np.random.randn(n_hidden)\n",
    "    \n",
    "with pm.Model() as neural_network:\n",
    "    # Weights from input to hidden layer\n",
    "    weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n",
    "                             shape=(n_feat, n_hidden), \n",
    "                             testval=init_1)\n",
    "    \n",
    "    # Weights from 1st to 2nd layer\n",
    "    weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n",
    "                            shape=(n_hidden, n_hidden), \n",
    "                            testval=init_2)\n",
    "    \n",
    "    # Weights from hidden layer to output\n",
    "    weights_2_out = pm.Normal('w_2_out', 0, sd=1, \n",
    "                              shape=(n_hidden,), \n",
    "                              testval=init_out)\n",
    "    \n",
    "    # Build neural-network using tanh activation function\n",
    "    act_1 = T.tanh(T.dot(ann_input, \n",
    "                         weights_in_1))\n",
    "    act_2 = T.tanh(T.dot(act_1, \n",
    "                         weights_1_2))\n",
    "    act_out = T.nnet.sigmoid(T.dot(act_2, \n",
    "                                   weights_2_out))\n",
    "    \n",
    "    # Binary classification -> Bernoulli likelihood\n",
    "    out = pm.Bernoulli('out', \n",
    "                       act_out,\n",
    "                       observed=ann_output.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average ELBO = -1.7065e+06: 100%|██████████| 50000/50000 [47:28<00:00, 18.74it/s]\n",
      "Finished [100%]: Average ELBO = -1.7064e+06\n"
     ]
    }
   ],
   "source": [
    "with neural_network:\n",
    "    # Run ADVI which returns posterior means, standard deviations, and the evidence lower bound (ELBO)\n",
    "    v_params = pm.variational.advi(n=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 5340.48it/s]\n"
     ]
    }
   ],
   "source": [
    "with neural_network:\n",
    "    trace = pm.variational.sample_vp(v_params, draws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:03<00:00, 130.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "[1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Accuracy = 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Replace shared variables with testing set\n",
    "ann_input.set_value(valData)\n",
    "ann_output.set_value(valLab)\n",
    "\n",
    "# Creater posterior predictive samples\n",
    "ppc = pm.sample_ppc(trace, model=neural_network, samples=500)\n",
    "\n",
    "# Use probability of > 0.5 to assume prediction of class 1\n",
    "pred = ppc['out'].mean(axis=0) > 0.5\n",
    "\n",
    "#print('Accuracy = {}%'.format((valLab == pred).mean() * 100))\n",
    "print [int(valLab[j][0].strip()) for j in range(len(valLab))]\n",
    "print list(pred*1)\n",
    "print('Accuracy = {}%'.format(compute_accuracy_val(valLab, list(pred*1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
